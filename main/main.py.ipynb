{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7358691,"sourceType":"datasetVersion","datasetId":4274196},{"sourceId":416857,"sourceType":"modelInstanceVersion","modelInstanceId":340086,"modelId":361206}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T11:56:47.218182Z","iopub.execute_input":"2025-05-29T11:56:47.218421Z","iopub.status.idle":"2025-05-29T11:57:14.963648Z","shell.execute_reply.started":"2025-05-29T11:56:47.218397Z","shell.execute_reply":"2025-05-29T11:57:14.96285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train=pd.read_csv(\"/kaggle/input/brand-sentiment-analysis-dataset/Dataset - Train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/brand-sentiment-analysis-dataset/Dataset - Test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T12:59:14.591401Z","iopub.execute_input":"2025-05-24T12:59:14.592046Z","iopub.status.idle":"2025-05-24T12:59:14.628582Z","shell.execute_reply.started":"2025-05-24T12:59:14.59202Z","shell.execute_reply":"2025-05-24T12:59:14.627839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[\"is_there_an_emotion_directed_at_a_brand_or_product\"] = df_train[\"is_there_an_emotion_directed_at_a_brand_or_product\"].replace(\"I can't tell\", \"No emotion toward brand or product\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T12:59:19.25191Z","iopub.execute_input":"2025-05-24T12:59:19.252183Z","iopub.status.idle":"2025-05-24T12:59:19.262795Z","shell.execute_reply.started":"2025-05-24T12:59:19.252165Z","shell.execute_reply":"2025-05-24T12:59:19.261998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlabel_encoder = LabelEncoder()\ndf_train[\"sentiment_encoded\"] = label_encoder.fit_transform(df_train[\"is_there_an_emotion_directed_at_a_brand_or_product\"])   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T12:59:24.538565Z","iopub.execute_input":"2025-05-24T12:59:24.538931Z","iopub.status.idle":"2025-05-24T12:59:24.546289Z","shell.execute_reply.started":"2025-05-24T12:59:24.538907Z","shell.execute_reply":"2025-05-24T12:59:24.545112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[\"sentiment_encoded\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:00:21.185291Z","iopub.execute_input":"2025-05-24T13:00:21.186079Z","iopub.status.idle":"2025-05-24T13:00:21.197727Z","shell.execute_reply.started":"2025-05-24T13:00:21.186051Z","shell.execute_reply":"2025-05-24T13:00:21.196951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nlpaug\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T11:57:30.200586Z","iopub.execute_input":"2025-05-29T11:57:30.200911Z","iopub.status.idle":"2025-05-29T11:57:38.079245Z","shell.execute_reply.started":"2025-05-29T11:57:30.200886Z","shell.execute_reply":"2025-05-29T11:57:38.078514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T11:58:05.771827Z","iopub.execute_input":"2025-05-29T11:58:05.772758Z","iopub.status.idle":"2025-05-29T11:58:22.206292Z","shell.execute_reply.started":"2025-05-29T11:58:05.772718Z","shell.execute_reply":"2025-05-29T11:58:22.20539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\n#from collections import counter\n\naugmenter= naw.SynonymAug(aug_src='wordnet', lang='eng')\nclass_counts= df_train[\"sentiment_encoded\"].value_counts()\nmax_class= class_counts.max()\nbalanced_data=[]\n\nfor label in class_counts.index:\n    class_df= df_train[df_train['sentiment_encoded']==label]\n    samples_needed=max_class-len(class_df)\n    balanced_data.append(class_df)\n    \n    if samples_needed > 0:\n        augmented_texts = []\n        while len(augmented_texts) < samples_needed:\n            for text in class_df['tweet_text']:\n                aug_text = augmenter.augment(text)\n                if aug_text != text:  # Avoid unchanged sentences\n                    augmented_texts.append(aug_text)\n                if len(augmented_texts) >= samples_needed:\n                    break\n\n        aug_df = pd.DataFrame({\n            'tweet_text': augmented_texts,\n            'sentiment_encoded': [label] * len(augmented_texts)\n        })\n        balanced_data.append(aug_df)\n\nbalanced_df = pd.concat(balanced_data, ignore_index=True)\n\n# Check final balance\nprint(balanced_df['sentiment_encoded'].value_counts())\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:07:17.879133Z","iopub.execute_input":"2025-05-24T13:07:17.879459Z","iopub.status.idle":"2025-05-24T13:07:27.856702Z","shell.execute_reply.started":"2025-05-24T13:07:17.879434Z","shell.execute_reply":"2025-05-24T13:07:27.855929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(\n    balanced_df[\"tweet_text\"].values, balanced_df[\"sentiment_encoded\"].values,\n    test_size=0.2,random_state=42,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:07:53.076964Z","iopub.execute_input":"2025-05-24T13:07:53.078747Z","iopub.status.idle":"2025-05-24T13:07:53.094569Z","shell.execute_reply.started":"2025-05-24T13:07:53.078707Z","shell.execute_reply":"2025-05-24T13:07:53.093662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:45:55.38412Z","iopub.execute_input":"2025-05-23T14:45:55.384807Z","iopub.status.idle":"2025-05-23T14:45:56.429338Z","shell.execute_reply.started":"2025-05-23T14:45:55.384781Z","shell.execute_reply":"2025-05-23T14:45:56.42856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n       text = str(self.texts[idx])  # Ensure text is a string\n       label = self.labels[idx]\n       encoding = self.tokenizer(\n        text,\n        add_special_tokens=True,\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n       return {\n        \"input_ids\": encoding[\"input_ids\"].squeeze(),\n        \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n        \"labels\": torch.tensor(label, dtype=torch.long)\n    }\n\nmax_len = 128\ntrain_dataset = ClassificationDataset(train_texts, train_labels, tokenizer, max_len)\nval_dataset = ClassificationDataset(val_texts, val_labels, tokenizer, max_len)\n\n# Create DataLoaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:45:58.727523Z","iopub.execute_input":"2025-05-23T14:45:58.727787Z","iopub.status.idle":"2025-05-23T14:45:58.734443Z","shell.execute_reply.started":"2025-05-23T14:45:58.727767Z","shell.execute_reply":"2025-05-23T14:45:58.733862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained( \"bert-base-uncased\",\n                                 num_labels=len(label_encoder.classes_))\nmodel.to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=7e-7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:46:02.699998Z","iopub.execute_input":"2025-05-23T14:46:02.700585Z","iopub.status.idle":"2025-05-23T14:46:05.232398Z","shell.execute_reply.started":"2025-05-23T14:46:02.700562Z","shell.execute_reply":"2025-05-23T14:46:05.231613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 18\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    correct=0\n    total=0\n    loop = tqdm(train_loader, leave=True)\n\n    for batch in loop:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        predictions = torch.argmax(outputs.logits, dim=1)\n\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n        loop.set_description(f\"Epoch {epoch}\")\n        loop.set_postfix(loss=loss.item())\n        accuracy = correct / total\n        \n    print(f\"Epoch {epoch} Loss: {total_loss / len(train_loader)}\")\n    print(f\"Epoch {epoch} accuracy: {accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:46:12.601461Z","iopub.execute_input":"2025-05-23T14:46:12.602326Z","iopub.status.idle":"2025-05-23T14:56:25.666723Z","shell.execute_reply.started":"2025-05-23T14:46:12.602296Z","shell.execute_reply":"2025-05-23T14:56:25.665941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nmodel.eval()\ncorrect = 0\ntotal = 0\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=1)\n\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n        \n        all_preds.extend(predictions.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \naccuracy = correct /total\nf1 = f1_score(all_labels, all_preds, average='weighted')  # or 'macro', 'micro', etc.\nf1_micro = f1_score(all_labels, all_preds, average='micro')\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"F1 score: {f1:.4f}\")\nprint(f\"F1 score: {f1_micro:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:22:11.625792Z","iopub.execute_input":"2025-05-23T11:22:11.626194Z","iopub.status.idle":"2025-05-23T11:23:13.737161Z","shell.execute_reply.started":"2025-05-23T11:22:11.626162Z","shell.execute_reply":"2025-05-23T11:23:13.736322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"bert_resume_classifier\")\ntokenizer.save_pretrained(\"bert_resume_classifier\")\ntorch.save(label_encoder, \"label_encoder.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:47:03.764595Z","iopub.execute_input":"2025-05-22T16:47:03.76552Z","iopub.status.idle":"2025-05-22T16:47:04.780318Z","shell.execute_reply.started":"2025-05-22T16:47:03.765476Z","shell.execute_reply":"2025-05-22T16:47:04.779734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}